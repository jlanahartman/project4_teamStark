# -*- coding: utf-8 -*-
"""Copy of Project 4_Collab - TEST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vIu2zpDzlAI0b6Qwn6o896Mzg8EUuy8l
"""

#import dependencies
import pandas as pd

#read the csv file
#wineDF = pd.read_csv("output_data/winemag-data-130k-v2.csv", encoding='utf8')
wineDF = pd.read_csv("winemag-data-130k-v2.csv")

#review the dataframe
wineDF

#removed 5 columns
wineDFCleaned = wineDF.drop(columns= ["taster_name","taster_twitter_handle", "Unnamed: 0", "region_2", "region_1"])

wineDFCleaned

#drop NA
wineDFCleaned = wineDFCleaned.dropna()

#review the cleaned dataframe
wineDFCleaned

#drop duplicates
wineDFCleaned = wineDFCleaned.drop_duplicates()

#review the cleaned dataframe
wineDFCleaned

#review the cleaned dataframe
wineDFCleaned

#reset index
wineDFReset = wineDFCleaned.reset_index()

#review the cleaned dataframe
wineDFReset

#removed 1 column
wineDFFinal = wineDFReset.drop(columns= "index")

wineDFFinal

#wineDFFinal.to_csv("wineDF.csv")

import os # allow for pyspark / hadoop / java instance components to be installed into the runtime
# set up a reference to the pyspark instance - stable version - 3.3.2
spark_version = 'spark-3.3.2'
# use os.environ function to reference 'SPARK_VERSION' property
os.environ['SPARK_VERSION'] = spark_version
# Install the spark and java components
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz
!tar xf $SPARK_VERSION-bin-hadoop3.tgz
!pip install -q findspark
# set the runtime environment variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64" # sets the path to the Java Virtual Machine (Java Development Kit)
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop3"
# start a spark session by importing and using findspark
import findspark
findspark.init()

# Import packages
from pyspark.sql import SparkSession
import time

# Create a SparkSession
spark = SparkSession.builder.appName("wineWords").getOrCreate()

from pyspark import SparkFiles

#add the file to the SparkFiles context
spark.sparkContext.addFile("wineDF.csv")

#make the df based on the dataset
wineDFSpark = spark.read.option('header', 'true').csv(
    SparkFiles.get("wineDF.csv"),
    inferSchema=True,
    sep=','
)
wineDFSpark.show(truncate=False)

# tokenize your data using the Tokenizer function
# import the Tokenizer module
from pyspark.ml.feature import Tokenizer

# create the tokenizer object
tokenizer = Tokenizer(inputCol="description", outputCol="tokens")

# transform the dataframe using the tokenizer object
tokenizedDF = tokenizer.transform(wineDFSpark)

# display the dataframe with the tokens
tokenizedDF.show(truncate=False)

# import StopWordsRemover module
from pyspark.ml.feature import StopWordsRemover

# make an instance of a general StopWordsRemover - removes words such as is, and, or, but, etc
remover01 = StopWordsRemover(inputCol="tokens", outputCol="filtered01")

# transform the data
filteredDF = remover01.transform(tokenizedDF)

# Show the DataFrame
filteredDF.show(truncate=False)

# Save as a new DataFrame, droping designation and tokens
resultDF = filteredDF.select( "_c0","country", "description",  "points", "price", "province", "title", "variety", "winery", "filtered01")

resultDF.show()

from pyspark.ml.feature import HashingTF, IDF, CountVectorizer

# make the count vectorizer object
cv = CountVectorizer(inputCol="filtered01", outputCol="features")

# fit the count vectorizer onto the dataframe
model = cv.fit(resultDF)

# use .transform to modify the dataframe with the count vectorized data
countVectorizedDF = model.transform(resultDF)

countVectorizedDF.show(truncate=False)

# hashing TF instance
ht = HashingTF(inputCol="filtered01", outputCol="hashingFeatures")

# use .transform on the countvectorized DF to compare
hashingCVDF = ht.transform(countVectorizedDF)

hashingCVDF.show(truncate=False)

# hashing TF instance
ht2 = HashingTF(inputCol="filtered01", outputCol="hashingFeatures2", numFeatures=16)

# use .transform on the countvectorized DF to compare
hashingCVDF2 = ht2.transform(hashingCVDF)

hashingCVDF2.show(truncate=False)

# IDF Demo
idf = IDF(inputCol="hashingFeatures2", outputCol="IDFFeatures")

# fit the IDF onto the dataframe
idfModel = idf.fit(hashingCVDF2)

# use .transform to rescale the hashingFeatures2 column (hashing with up to 16 features)
rescaledHashingCVDF = idfModel.transform(hashingCVDF2)

rescaledHashingCVDF.show(truncate=False)

#2 way of dealing with resultDF
# Create a column that adds the length of the review as a feature.
# count the number of characters in the column
from pyspark.sql.functions import length

# add the length column to the reviewDF DataFrame
reviewDF = resultDF.withColumn('length', length(resultDF['description']))

reviewDF.show(truncate=False)

# import neccessary modules for the transformations
from pyspark.ml.feature import StringIndexer, Tokenizer, StopWordsRemover, HashingTF, IDF

# make the features as the objects / variables that will hold the transformations to the dataFrame
changePosAndNeg = StringIndexer(inputCol='country', outputCol='label') # create a new column where 1 = positive, and 0 = negative
tokenizer = Tokenizer(inputCol='description', outputCol='tokens') # makes a column named 'tokens' that tokenizes the data
stopRemover = StopWordsRemover(inputCol='tokens', outputCol='filtered') # make a column named 'filtered' with the filtered text w/ removed stopwords
hashingTF = HashingTF(inputCol='filtered', outputCol='hashedFeatures') # make a column named 'hashedFeatures' with the hashingTF results based on filtered text
idf = IDF(inputCol='hashedFeatures', outputCol='idfFeatures') # make a column named 'idfFeatures' that calculates the idf features based on the hashing tf results

# import the vector modules
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.linalg import Vector

# make the final feature vectors
finalStep = VectorAssembler(inputCols=['idfFeatures', 'length'], outputCol='features')

# make the processing pipeline
from pyspark.ml import Pipeline

# make an object that processes all previous steps
dataPipeline = Pipeline(stages=[changePosAndNeg, tokenizer, stopRemover, hashingTF, idf, finalStep])

# fit the data onto the reviewDF dataframe
reviewPipeline = dataPipeline.fit(reviewDF)

# transform the data and make a new dataFrame
processedReviewDF = reviewPipeline.transform(reviewDF)

# view the first 5 rows
processedReviewDF.show(5)

# use .randomsplit to separate the data into tranining and testing
#training, testing = rescaledHashingCVDF.randomSplit([0.75, 0.25]) -- why this was used in the class example???
training, testing = processedReviewDF.randomSplit([0.75, 0.25])

# import Naive Bayes classifier
from pyspark.ml.classification import NaiveBayes

# make the NB model object
nb = NaiveBayes()

# make a predictor object that is fit onto the training data
nbPredictor = nb.fit(training)

# use the testing data to generate test results
nBTestResults = nbPredictor.transform(testing)

# see the first 5 rows (look at the last column)
nBTestResults.show(5)

# use the Multi Class Classification evaluator to see prediction accuracy
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# make the evaluator object
accuracyEval = MulticlassClassificationEvaluator()

# evaluate the results (produces an accuracy percentage)
accuracy = accuracyEval.evaluate(nBTestResults)

# show the accuracy
print(f"Accuracy of the model at predicting the country of the wine review: {accuracy*100:.2f}%")

model.save('naive_bayes_model')

from pyspark.sql import Row

# Prepare your input data
inputWord = "iconic"
inputRow = Row("filtered01")(inputWord)
inputDF = spark.createDataFrame([inputRow])

from pyspark.sql.functions import split

# Convert filtered01 column from string to array<string>
filteredDF = filteredDF.withColumn("filtered01", split(filteredDF["filtered01"], " "))

# Print the resulting DataFrame
filteredDF.show(truncate=False)

predictions = model.transform(inputDF)

predictions.select("filtered01", "prediction").show()

"""TO DO:

1) clean up to remove comma and anything else messing up with the accuracy.

2) Add a second layer of stopwords to remove words that dont make sense

3) Binding to 8 varieties and evrything else is "other"

43)Change the prediction from country to grape variety
Now: the words(description) is predicting  country
Future: the words(description) is predicting  grape variety

5) Make sure the model is correct
-> test inputting words and see if the correct variety is coming out

6) website file to be uploaded
"""